{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7efe5b00-cda0-4c3f-9cbc-6fd590ebb4a6",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "22382432-34a8-474b-9519-af1168597ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b52f74-b11f-4edc-9dc1-af4d396cce68",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a84e42f-922c-4060-a00b-85dd862b1e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_en = [\n",
    "    \"good morning\",\n",
    "    \"ai books\"    \n",
    "]\n",
    "\n",
    "# max vocabulary size and sequence length\n",
    "vocab_size_en = 7\n",
    "sequence_length_en = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "838ccb88-1930-47ba-8536-c97bb58b9fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer and define a trainer\n",
    "tokenizer_en = Tokenizer(WordLevel())\n",
    "tokenizer_en.pre_tokenizer = Whitespace()\n",
    "tokenizer_en.enable_padding(pad_id=1, \n",
    "                                    pad_token=\"<pad>\", \n",
    "                                    length=sequence_length_en)\n",
    "tokenizer_en.enable_truncation(max_length=sequence_length_en)\n",
    "\n",
    "# Train the tokenizer on your corpus\n",
    "trainer_generation = WordLevelTrainer(vocab_size=vocab_size_en, \n",
    "                                      special_tokens=[\"<unk>\", \"<pad>\", \"<eos>\"])\n",
    "tokenizer_en.train_from_iterator(corpus_en, trainer_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "53c1877c-fd26-4a6f-9bd3-ff50a1e3ab0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[5, 6, 1],\n",
      "        [3, 4, 1]])\n",
      "{'ai': 3, '<eos>': 2, '<unk>': 0, 'morning': 6, '<pad>': 1, 'books': 4, 'good': 5}\n"
     ]
    }
   ],
   "source": [
    "topics_ids = []\n",
    "for x in corpus_en:\n",
    "    x_ids = tokenizer_en.encode(x).ids\n",
    "    topics_ids.append(x_ids)\n",
    "\n",
    "en_data = torch.tensor(topics_ids, dtype=torch.long)\n",
    "\n",
    "print(en_data)\n",
    "print(tokenizer_en.get_vocab())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9221da0e-a1c6-4944-b4a8-05b7f66b3cd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "164459c8-b226-447a-8a69-4a12e05e7566",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus_vn = [\n",
    "    \"chào buổi sáng\",\n",
    "    \"sách ai\"    \n",
    "]\n",
    "\n",
    "# max vocabulary size and sequence length\n",
    "vocab_size_vn = 9\n",
    "sequence_length_vn = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2726650a-52ef-4150-9b09-0071d0ce2a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer and define a trainer\n",
    "tokenizer_vn = Tokenizer(WordLevel())\n",
    "tokenizer_vn.pre_tokenizer = Whitespace()\n",
    "tokenizer_vn.enable_padding(pad_id=1, \n",
    "                            pad_token=\"<pad>\", \n",
    "                            length=sequence_length_vn)\n",
    "tokenizer_vn.enable_truncation(max_length=sequence_length_vn)\n",
    "\n",
    "# Train the tokenizer on your corpus\n",
    "trainer_vn = WordLevelTrainer(vocab_size=vocab_size_vn, \n",
    "                              special_tokens=[\"<unk>\", \"<pad>\", \"<sos>\", \"<eos>\"])\n",
    "tokenizer_vn.train_from_iterator(corpus_vn, trainer_vn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "548484cb-e415-4e4f-a7f5-b0fd0ed48e59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos> chào buổi sáng', '<sos> sách ai']\n",
      "['chào buổi sáng <eos>', 'sách ai <eos>']\n"
     ]
    }
   ],
   "source": [
    "data_x = []\n",
    "data_y = []\n",
    "for vector in corpus_vn:\n",
    "    vector = ['<sos>'] + vector.split() + ['<eos>']\n",
    "    data_x.append( ' '.join(vector[:-1]) )\n",
    "    data_y.append( ' '.join(vector[1:]) )\n",
    "\n",
    "print(data_x)\n",
    "print(data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d28fcc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2, 6, 5, 8] [6, 5, 8, 3]\n",
      "[2, 7, 4, 1] [7, 4, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize and numericalize your samples\n",
    "def vectorize_generation(x, y, tokenizer_vn):     \n",
    "    x_ids = tokenizer_vn.encode(x).ids\n",
    "    y_ids = tokenizer_vn.encode(y).ids\n",
    "    print(x_ids, y_ids)\n",
    "    return x_ids, y_ids\n",
    "\n",
    "# Vectorize the samples\n",
    "input_vn_data = []\n",
    "label_vn_data = []\n",
    "for x, y in zip(data_x, data_y):\n",
    "    x_ids, y_ids = vectorize_generation(x, y, tokenizer_vn)\n",
    "    input_vn_data.append(x_ids)\n",
    "    label_vn_data.append(y_ids)\n",
    "\n",
    "input_vn_data = torch.tensor(input_vn_data, dtype=torch.long)\n",
    "label_vn_data = torch.tensor(label_vn_data, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b547154b-2223-4e4f-a875-119ec33f9812",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "72f159d4-4a24-4cd2-bd45-853802e1ba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size_en, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size_en, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "\n",
    "    # src: [batch_size, seq_length]\n",
    "    def forward(self, src): \n",
    "        embedded = self.embedding(src)  # [batch_size, seq_length, embedding_dim]        \n",
    "        _, hidden = self.rnn(embedded)  # [1, batch_size, hidden_dim]        \n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c940aa66-82d6-448e-a349-2ed223cd065c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 6])\n"
     ]
    }
   ],
   "source": [
    "embedding_dim, hidden_dim = 6, 6\n",
    "encoder = Encoder(vocab_size_en, embedding_dim, hidden_dim)\n",
    "\n",
    "hidden_sample = encoder(en_data)\n",
    "print(hidden_sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01e38434-1d7a-4c22-b278-ec03481377ea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39687bf5-3741-4910-8444-c89358bc81f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size_vn, embedding_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size_vn, embedding_dim)\n",
    "        self.rnn = nn.RNN(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hidden_dim, vocab_size_vn)\n",
    "\n",
    "    # input: [batch_size, seq_length]\n",
    "    # hidden: [1, batch_size, hidden_dim]\n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)          # [batch_size, seq_length, embedding_dim]        \n",
    "        output, _ = self.rnn(embedded, hidden)    # [batch_size, seq_length, hidden_dim]\n",
    "        prediction = self.fc_out(output)          # [batch_size, vocab_size_vn]\n",
    "        \n",
    "        return prediction.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ae940e0-e7bf-43c3-af4c-45ea79ffac7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 9, 4])\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(vocab_size_vn, embedding_dim, hidden_dim)\n",
    "outputs = decoder(input_vn_data, hidden_sample)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a328c8e3-1ed9-4df0-9bc5-f65a0871c47b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0eb95237-d3b6-4f42-9f3e-25aadd967a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq_Model(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, sequence_en, sequence_vn):        \n",
    "        hidden = self.encoder(sequence_en)\n",
    "        outputs = self.decoder(sequence_vn, hidden)\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f7cb8b50-51a1-43b1-9d2d-569c269e3ee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seq2Seq_Model(\n",
      "  (encoder): Encoder(\n",
      "    (embedding): Embedding(7, 6)\n",
      "    (rnn): RNN(6, 6, batch_first=True)\n",
      "  )\n",
      "  (decoder): Decoder(\n",
      "    (embedding): Embedding(9, 6)\n",
      "    (rnn): RNN(6, 6, batch_first=True)\n",
      "    (fc_out): Linear(in_features=6, out_features=9, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = Seq2Seq_Model(encoder, decoder)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1367b830-30f1-43e1-bef1-e896633345ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 9, 4])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(en_data, input_vn_data)\n",
    "print(outputs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec415bd4-83c5-44f1-9be5-40e7b46a82e4",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32ed2deb-b3e3-4a70-8b85-c575001fc213",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cbee29f3-5988-4f5f-a9f1-fb13f137a91d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(50):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(en_data, input_vn_data)\n",
    "    loss = criterion(outputs, label_vn_data)\n",
    "    #print(loss.item())\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b562b790-b1f8-4e0c-a1cc-2453ee3f3a1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6, 5, 8, 3],\n",
      "        [7, 4, 3, 1]])\n"
     ]
    }
   ],
   "source": [
    "outputs = model(en_data, input_vn_data)\n",
    "print(torch.argmax(outputs, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "051f948f-3123-43f0-8e9d-ec8315314d17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6, 5, 8, 3],\n",
       "        [7, 4, 3, 1]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_vn_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f519bd6-a4f9-4037-bea8-bf7118e684f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
